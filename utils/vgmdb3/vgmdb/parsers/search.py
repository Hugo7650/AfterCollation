import bs4
import re
import urllib.request, urllib.parse, urllib.error

from . import utils

# class AppURLOpener(urllib.request.FancyURLopener):
# 	version = "vgmdbapi/0.2 +https://vgmdb.info"
# urllib.request._opener = AppURLOpener()

def fetch_url(query):
	return 'https://vgmdb.net/search?q=%s'%(urllib.parse.quote(query))
def fetch_page(query):
	url = fetch_url(query)
	page = urllib.request.urlopen(url)
	if page.geturl() == url:
		data = page.read()
		data = data.decode('utf-8', 'ignore')
		return data
	return masquerade(url, page)

def masquerade(url, page):
	import urllib.parse
	import importlib
	sections = {'albums':[],
	            'artists':[],
	            'orgs':[],
	            'products':[]}
	parsed = urllib.parse.urlparse(page.geturl())
	data = page.read()
	data = data.decode('utf-8', 'ignore')
	for section in list(sections.keys()):
		type = section[:-1]
		prefix = '/%s/'%type
		if parsed.path[:len(prefix)] == prefix:
			# module = importlib.import_module('.parsers.'+type, 'vgmdb')
			module = importlib.import_module('utils.vgmdb3.vgmdb.parsers.'+type)
			parse_page = getattr(module, "parse_page")
			info = parse_page(data)
			info['link'] = parsed.path[1:]
			fake = generate_fakeresult(info)
			sections[section].append(fake)
	orig_parsed = urllib.parse.urlparse(url)
	query = urllib.parse.parse_qs(orig_parsed.query)['q'][0]
	query = urllib.parse.unquote(query)
	fake = {
	    "meta":{},
	    "query":query,
	    "results":sections,
	    "sections":sorted(sections.keys())
	}
	return fake

def generate_fakeresult(info):
	fake = {'link':info['link']}
	copy_keys = ['aliases', 'category', 'catalog', 'media_format', 'release_date']
	for key in copy_keys:
		if key in info:
			fake[key] = info[key]
	if 'names' in info:
		fake['titles'] = info['names']
	if not 'names' in info:
		if 'name' in info:
			fake['names'] = {}
			fake['names']['en'] = info['name']
		if 'name_real' in info:
			fake['names']['ja'] = info['name_real']
			fake['aliases'] = [info['name_real']]
			fake['names']['ja-latn'] = info['name']
		if 'name_trans' in info:
			fake['names']['ja-latn'] = info['name_trans']
	return fake

def parse_page(html_source):
	if isinstance(html_source, dict):
		# fake result generated by redirect
		return html_source
	section_types = {
		'albumresults': 'albums',
		'artistresults': 'artists',
		'orgresults': 'orgs',
		'productresults': 'products'
	}

	search_info = {}
	search_info['results'] = {}
	search_info['sections'] = []
	html_source = utils.fix_invalid_table(html_source)
	soup = bs4.BeautifulSoup(html_source, features='html.parser')
	soup_innermain = soup.find(id='innermain')
	if soup_innermain == None:
		return {}	# info not found

	# parse the section
	for soup_section in soup_innermain.div.find_all('div'):
		if not soup_section.has_attr('id'):
			continue
		section_type = soup_section['id']
		if section_type not in section_types:
			continue
		section_type = section_types[section_type]
		parse_item = globals()['_parse_'+section_type[:-1]]
		search_info['sections'].append(section_type)
		search_info['results'][section_type] = _parse_list(soup_section, parse_item)

	# parse the query
	match = re.search(r'\$\("#simplesearch"\).val\("(.*)"\);', html_source)
	if match:
		search_info['query'] = match.groups(1)[0].replace("\\'","'")

	# parse page meat
	search_info['meta'] = {}

	return search_info

def _parse_list(soup_section, item_parser):
	list = []
	for soup_row in soup_section.find_all('tr', recursive=True)[1:]:
		item = item_parser(soup_row)
		if item:
			list.append(item)
	return list

def _parse_listitem(soup_row):
	soup_cells = soup_row.find_all('td', recursive=False)
	soup_link = soup_cells[0].a
	names = utils.parse_names(soup_link)
	link = soup_link['href']
	link = utils.trim_absolute(link)
	info = {'link':link,
	        'names':names
	}
	# aliases could be aliases or name_trans, dunno
	soup_aliases = soup_cells[0].span
	if soup_aliases:
		aliases = utils.parse_string(soup_aliases)
		aliases = [piece.strip() for piece in aliases.split('/') if piece.strip()!='']
		info['aliases'] = aliases
	return info

def _parse_album(soup_row):
	soup_cells = soup_row.find_all('td', recursive=False)
	catalog = str(soup_cells[0].span.string)
	special = soup_cells[1].img
	soup_album = soup_cells[2]
	link = soup_album.a['href']
	link = utils.trim_absolute(link)
	names = utils.parse_names(soup_album.a)
	date = utils.parse_date_time(soup_cells[3].string)
	media_format = str(soup_cells[4].string)
	info = {'link':link,
	        'catalog':catalog,
	        'titles':names,
	        'release_date':date,
	        'media_format':media_format
	}
	typelist = [s.replace('album-','') for s in soup_album.a['class'] if 'album-' in s]
	info['category'] = utils.type_category(typelist[0])
	return info
_parse_artist = _parse_listitem
_parse_org = _parse_listitem
def _parse_product(soup_row):
	soup_cells = soup_row.find_all('td', recursive=False)
	soup_link = soup_cells[0].a
	names = utils.parse_names(soup_link.span.span)
	link = soup_link['href']
	link = utils.trim_absolute(link)
	info = {'link':link}
	# aliases could be aliases or name_trans, dunno
	soup_color = soup_cells[0].span
	if soup_color:
		info['type'] = utils.product_color_type(soup_color)
		soup_names = soup_color.span
		if soup_names:
			names = utils.parse_names(soup_color)
			info['names'] = names
	return info
